{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic POlytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SDGAI_LLMforGenAIApp_Labs/blob/main/L01/L01.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `c3669c`.\n",
    "* Pip install the libraries stated below when necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a896f4-8509-456a-9a25-46532342f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118fabe-37b7-4cd4-b7a4-9b0fc3875ca3",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This step takes a piece of text and converts it into a list of tokens. If the input is a sentence, then separating the words (including punctuations) would be an example of tokenization. Depending on the model, different granularities can be chosen. At the lowest level, each character could be a token.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3810241-4afe-4e4b-881b-8df5bd5b5034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\koay_seng_tian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# download and install the resource \n",
    "# https://www.nltk.org/api/nltk.tokenize.punkt.html\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589a4cdc-02e5-4f6d-a820-7d40dbf3becf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"I am learning natural language processing.\")\n",
    "\n",
    "# print the tokens\n",
    "# notice punctuations like full stop is a token\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7927b0-9909-4e54-b997-ac49c1aeaa09",
   "metadata": {},
   "source": [
    "## PoS (Part-of-Speech) Tagging\n",
    "\n",
    "PoS refers to parts of speech. PoS tagging refers to the process of tagging words within sentences into their respective parts of speech and then labelling them.\n",
    "\n",
    "|     | **NLTK** (not exhaustive)                                           |\n",
    "|-----|---------------------------------------------------------------------|\n",
    "| DT  | Determiner                                                          |\n",
    "| JJ  | Adjuctive                                                           |\n",
    "| NN  | Noun, common, singular or mass                                      |\n",
    "| PRP | Personal pronoun (e.g. he, she I, you)                              |\n",
    "| VBG | Verb, gerund or present participate (e.g. running, eating, walking) |\n",
    "| VBP | Verb, present tense (e.g. run, eat, walk)                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ff80c2-194b-4ef6-ae57-d3abbee5dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\koay_seng_tian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# download and install the resource \n",
    "# https://www.nltk.org/_modules/nltk/tag/perceptron.html\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "581a6563-51f4-4d53-8edf-7abd9617cebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'natural', 'language', 'processing', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenize the sentence and print the tokens\n",
    "words = word_tokenize(\"I am learning natural language processing.\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "046a1268-24d1-4b18-8f04-15720dea6810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('learning', 'VBG'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the PoS tags\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c62102-2fc4-420a-98cf-cd06aed03f1d",
   "metadata": {},
   "source": [
    "## Stop Word\n",
    "\n",
    "Stop words are common words that are just used to support the sentence construction. Stop words are removed from our analysis as they do not impact the meaning of sentence they are present in. Examples of stop words include `a`, `am` and `the`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd5f12f-c3fd-4e13-bc0b-f93e79ea5c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\koay_seng_tian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download and install the resource \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591d3d22-f8d7-43db-a5d5-938617afbb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# to print the list of stopwords in English language\n",
    "stop_words = stopwords.words('English')\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53895d7a-ff5a-4948-a97f-1b569758c15c",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77c1d977-21b6-4c6c-a99a-655a6b4f72cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am learning Python. It is one of the most popular programming language.\"\n",
    "sentence_words = word_tokenize(sentence)\n",
    "\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15ff08ed-c9a4-4b8b-9c8a-7e99070e80b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming language .\n"
     ]
    }
   ],
   "source": [
    "# the code below uses python list comprehension to construct the sentence_no_stop_word list\n",
    "sentence_no_stop_word = ' '.join([word for word in sentence_words if word not in stop_words])\n",
    "\n",
    "print(sentence_no_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e97c50-52cf-44fb-a761-83b40d23acad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming language .\n"
     ]
    }
   ],
   "source": [
    "# the code below is the same as above code except it is not using 'python list comprehension' technique\n",
    "sentence_no_stop_word = []\n",
    "\n",
    "for word in sentence_words:\n",
    "    if word in stop_words:\n",
    "        pass\n",
    "    else:\n",
    "        sentence_no_stop_word.append(word)\n",
    "\n",
    "print(' '.join(sentence_no_stop_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234c7e5-0438-402d-8cb9-9b313886a734",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "In English, words get tranformed into various forms when being in a sentence. For example, the word `product` get tranformed into `production`. It is necessary to convert these words into their base forms as they carry the same meaning.\n",
    "\n",
    "|            | Stemming Process |         |\n",
    "|------------|:----------------:|---------|\n",
    "| Production |        =>        | product |\n",
    "| Products   |        =>        | product |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ba4833-6c50-4216-b130-4fef6249b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6fc5e2b-00e1-41c3-9a9e-618d09f38d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product\n",
      "product\n",
      "come\n",
      "fire\n",
      "battl\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"Production\"))\n",
    "print(stemmer.stem(\"Products\"))\n",
    "print(stemmer.stem(\"coming\"))\n",
    "print(stemmer.stem(\"firing\"))\n",
    "\n",
    "print(stemmer.stem(\"battling\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b57cb08-395c-4862-91fe-88673c65766d",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "The stemming process, sometimes, leads to unusual result.  For example, the word 'battling' is transformed to 'battl\" which cannot be found in a dictionary. To overcome this issue, lemmatization is another technique to use. The base form of the word can be found in a dictionary. This additional check (is it a word in the dictionary?) slows down the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64933b80-43f4-4bcb-8d31-f9ef0c002814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\koay_seng_tian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f8fb2fc-ac63-42d5-b37f-3781a15fa7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30cced34-9c4d-45c0-ab8c-fcf1bd7e21bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production\n",
      "Products\n",
      "coming\n",
      "firing\n",
      "battling\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"Production\"))\n",
    "print(lemmatizer.lemmatize(\"Products\"))\n",
    "print(lemmatizer.lemmatize(\"coming\"))\n",
    "print(lemmatizer.lemmatize(\"firing\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"battling\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07db20b-9251-4488-a6f8-72822c7f55a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
