{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SDGAI_LLMforGenAIApp_Labs/blob/main/L01/L01.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `c3669c`.\n",
    "* Pip install the libraries stated below when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a896f4-8509-456a-9a25-46532342f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U ntlk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118fabe-37b7-4cd4-b7a4-9b0fc3875ca3",
   "metadata": {},
   "source": [
    "# Lesson 1\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "This step takes a piece of text and converts it into a list of tokens. If the input is a sentence, then separating the words (including punctuations) would be an example of tokenization. Depending on the model, different granularities can be chosen. At the lowest level, each character could be a token.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7927b0-9909-4e54-b997-ac49c1aeaa09",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "When we define a LangGraph `StateGraph`, we use a [state schema](https://langchain-ai.github.io/langgraph/concepts/low_level/#state).\n",
    "\n",
    "The state schema represents the structure and types of data that our graph will use.\n",
    "\n",
    "All nodes are expected to communicate with that schema.\n",
    "\n",
    "LangGraph offers flexibility in how you define your state schema, accommodating various Python [types](https://docs.python.org/3/library/stdtypes.html#type-objects) and validation approaches!\n",
    "\n",
    "## TypedDict\n",
    "\n",
    "As we mentioned in Module 1, we can use the `TypedDict` class from python's `typing` module.\n",
    "\n",
    "It allows you to specify keys and their corresponding value types.\n",
    " \n",
    "But, note that these are type hints. \n",
    "\n",
    "They can used by static type checkers (like [mypy](https://github.com/python/mypy)) or IDEs to catch potential type-related errors before the code is run. \n",
    "\n",
    "But they are not enforced at runtime!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
