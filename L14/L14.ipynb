{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1dd8cc-9979-4757-b5f5-21e4d5605d41",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SDGAI_LLMforGenAIApp_Labs/blob/main/L14/L14.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e66413-b20b-45dc-88a7-725a8005916d",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `c3669c`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e181688a-e654-416d-b660-d6e876f047e7",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e0e74-9750-484e-85a3-0e2df89ab3eb",
   "metadata": {},
   "source": [
    "# Lesson 14\n",
    "\n",
    "The code below demonstrates the LangSmith function using a simple Retrieval-Augmented Generation (RAG) system. It begins by building a vector store through the retrieval of content from a website's sitemap, such as https://docs.smith.langchain.com/sitemap.xml. A sitemap is a structured file or page that provides detailed information about the content of a website, enabling search engines like Google to understand its structure more effectively.\n",
    "\n",
    "This process constitutes the \"retrieval\" phase of a RAG system. The retrieved content is chunked into smaller segments and stored in a directory named `chroma_langsmith_db`. The next step involves answer generation. The RAG system leverages the LangChain Expression Language (LCEL) to accomplish this.\n",
    "\n",
    "As the Python code executes, observe that the response output is printed. However, if you are using LangSmith, manual output printing for tracing or debugging purposes is unnecessary, as LangSmith automatically facilitates these functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7a1281-5590-4f9d-88fc-76185733bf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langchain-openai\n",
    "%pip install --quiet -U langchain-community\n",
    "%pip install --quiet -U langsmith\n",
    "%pip install --quiet -U \"langchain-chroma>=0.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed6b64-6afd-49b1-b20d-c0126966cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain              0.3.11\n",
    "# langgraph              0.2.59\n",
    "# langchain-core         0.3.24\n",
    "# langchain-openai       0.2.12\n",
    "# langchain-community    0.3.12\n",
    "# openai                 1.57.2\n",
    "# pydantic               2.10.3\n",
    "# tavily-python          0.5.0\n",
    "# wikipedia              1.4.0\n",
    "# chroma                 0.5.23\n",
    "# PyMuPDF                1.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ddc59c-44d5-40e6-9f4e-cea7ee8b7c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "204b5efe-52a1-473b-baaf-49428f5c8d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f071c5-e8da-43b7-aae8-1c44101a645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "# setup the LangChain LangSmith API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]= \"LangSmith-Lesson-14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb0ab2c-c9c7-485b-9b33-d625b36f0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the python code below is to stop the warning message from showing up\n",
    "# USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
    "os.environ['USER_AGENT'] = \"my_user_agent\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2c924-3ffd-495a-95f6-5b64468cede9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e8de6c-7913-4ce7-ad37-71638acca457",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.sitemap import SitemapLoader\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters.character import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84c13169-e2fd-4684-89d5-fa49e9b06010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the model\n",
    "\n",
    "llm_model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a0ae01a-a8bb-4919-b21d-aa4cb163092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below python code is needed or you will get error:\n",
    "# asyncio.run() cannot be called from a running event loop\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "169e7ba7-40e2-4630-9ae9-21a61af53a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the vector store\n",
    "\n",
    "chroma_path = \"chroma\"\n",
    "collection_name = \"rag-chroma-langsmith\"\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def get_vector_db_retriever():\n",
    "\n",
    "    persist_directory = \"./chroma_langsmith_db\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "    if os.path.exists(persist_directory):\n",
    "        vector_store = Chroma(\n",
    "            persist_directory = persist_directory,\n",
    "            embedding_function = embeddings,\n",
    "            collection_name = collection_name\n",
    "        )\n",
    "\n",
    "        return vector_store.as_retriever()\n",
    "    \n",
    "    ls_docs_sitemap_loader = SitemapLoader(web_path=\"https://docs.smith.langchain.com/sitemap.xml\")\n",
    "    ls_docs = ls_docs_sitemap_loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, \n",
    "        chunk_overlap=100\n",
    "    )\n",
    "\n",
    "    texts = text_splitter.split_documents(ls_docs)\n",
    "\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents = texts, \n",
    "        embedding=embeddings,\n",
    "        persist_directory = persist_directory,\n",
    "        collection_name = collection_name\n",
    "    )\n",
    "\n",
    "    print(\"Vector store created\")\n",
    "\n",
    "    return vector_store.as_retriever()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c7d5a5b-5bba-43ab-84b6-bb624717082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to use model_dump to serialize <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> to JSON: PydanticSerializationError(Unable to serialize unknown type: <class 'langchain_chroma.vectorstores.Chroma'>)\n"
     ]
    }
   ],
   "source": [
    "# it takes some time to create the vector store if you running this first time\n",
    "# else the vector store is already created, it will load from existing vector db\n",
    "\n",
    "retriever = get_vector_db_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dcfcd6a-ec7a-4464-a4a2-4746b4127f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test retrieval by invoking it\n",
    "vector_db_response = retriever.invoke(\"What is LangSmith used for?\")\n",
    "\n",
    "# print the contexts retrieved\n",
    "docs = \"\"\n",
    "for index, doc in enumerate(vector_db_response):\n",
    "    docs = f'{docs}\\n{index}\\n--------------\\n{doc.page_content}\\n--------------\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2ee4b69-0231-48e8-94a7-76daad66d713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "--------------\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "With LangSmith you can:\n",
      "--------------\n",
      "\n",
      "1\n",
      "--------------\n",
      "LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, well highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if theyre just starting their journey.\n",
      "--------------\n",
      "\n",
      "2\n",
      "--------------\n",
      "LangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain's open source frameworks langchain and langgraph, with no extra instrumentation needed.If you're already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "LangSmith is a standalone platform that can be used on it's own no matter how you're creating your LLM applicatons.\n",
      "--------------\n",
      "\n",
      "3\n",
      "--------------\n",
      "Scripts for administering LangSmith | 锔锔 LangSmith\n",
      "--------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the output below is meant for observation and comparison later\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6527d99d-1931-4615-909c-005f9aaec85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up prompt template\n",
    "\n",
    "rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "human_prompt=\"\"\"\n",
    "{formatted_docs}\n",
    "\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", rag_system_prompt),\n",
    "    (\"human\", human_prompt)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ea02b4-05ec-4473-830d-d9cf8d14ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the chain using LCEL\n",
    "\n",
    "rag_chain = rag_prompt | llm_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cae200da-e476-464b-b631-2917ff6c9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    response = rag_chain.invoke({\n",
    "        \"formatted_docs\" : formatted_docs,\n",
    "        \"question\" : question\n",
    "    })\n",
    "\n",
    "    print(f\"1. generate_response: formatted_docs\\n{formatted_docs}\")\n",
    "    print(\"-\"*10)\n",
    "    print(f\"2. generate_response: question\\n{question}\")\n",
    "    print(\"-\"*10)\n",
    "    print(f\"3. generate_response: responese\\n{response}\")        \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9a28b7-8e55-4df9-8897-f1f3015fe42a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc0369c4-4269-4744-8940-4ebcd96603ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. generate_response: formatted_docs\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "With LangSmith you can:\n",
      "\n",
      "LangSmith is a platform for LLM application development, monitoring, and testing. In this guide, well highlight the breadth of workflows LangSmith supports and how they fit into each stage of the application development lifecycle. We hope this will inform users how to best utilize this powerful platform or give them something to consider if theyre just starting their journey.\n",
      "\n",
      "LangSmith + LangChain OSSLangSmith integrates seamlessly with LangChain's open source frameworks langchain and langgraph, with no extra instrumentation needed.If you're already using either of these, see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "LangSmith is a standalone platform that can be used on it's own no matter how you're creating your LLM applicatons.\n",
      "\n",
      "Scripts for administering LangSmith | 锔锔 LangSmith\n",
      "----------\n",
      "2. generate_response: question\n",
      "What is LangSmith used for?\n",
      "----------\n",
      "3. generate_response: responese\n",
      "LangSmith is used for building production-grade LLM applications, enabling users to monitor, evaluate, and test their applications effectively. It supports various workflows throughout the application development lifecycle. Additionally, LangSmith integrates with LangChain and LangGraph, but can also be used as a standalone platform.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is LangSmith used for?\"\n",
    "\n",
    "response = generate_response(question, vector_db_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f511c49e-cc0f-4455-aef5-8fdf1e43fd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is used for building production-grade LLM applications, enabling users to monitor, evaluate, and test their applications effectively. It supports various workflows throughout the application development lifecycle. Additionally, LangSmith integrates with LangChain and LangGraph, but can also be used as a standalone platform.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1f4cd-8603-426a-83ff-bfde1d75af3e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
