{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256d3948",
   "metadata": {},
   "source": [
    "<img src=\"https://www.rp.edu.sg/images/default-source/default-album/rp-logo.png\" width=\"200\" alt=\"Republic Polytechnic\"/>\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/koayst-rplesson/SDGAI_LLMforGenAIApp_Labs/blob/main/L09/L09.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e80164-bf97-4962-b425-7864a154dcaf",
   "metadata": {},
   "source": [
    "# Setup and Installation\n",
    "\n",
    "You can run this Jupyter notebook either on your local machine or run it at Google Colab.\n",
    "\n",
    "* For local machine, it is recommended to install Anaconda and create a new development environment called `c3669c`.\n",
    "* Pip/Conda install the libraries stated below when necessary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6f4c2-8506-415c-a295-041ee40d9eda",
   "metadata": {},
   "source": [
    "# <font color='red'>ATTENTION</font>\n",
    "\n",
    "## Google Colab\n",
    "- If you are running this code in Google Colab, **DO NOT** store the API Key in a text file and load the key later from Google Drive. This is insecure and will expose the key.\n",
    "- **DO NOT** hard code the API Key directly in the Python code, even though it might seem convenient for quick development.\n",
    "- You need to enter the API key at python code `getpass.getpass()` when ask.\n",
    "\n",
    "## Local Environment/Laptop\n",
    "- If you are running this code locally in your laptop, you can create a env.txt and store the API key there.\n",
    "- Make sure env.txt is in the same directory of this Jupyter notebook.\n",
    "- You need to install `python-dotenv` and run the Python code to load in the API key.\n",
    "\n",
    "---\n",
    "```\n",
    "%pip install python-dotenv\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('env.tx')\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "```\n",
    "---\n",
    "\n",
    "## GitHub/GitLab\n",
    "- **DO NOT** `commit` or `push` API Key to services like GitHub or GitLab.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c79ed1d-cc1e-4712-93ce-695496907dd7",
   "metadata": {},
   "source": [
    "# Lesson 09\n",
    "\n",
    "- LangChain is a framework built around LLMs.\n",
    "- Framework offered as a Python or Javascript (Typescript) package.\n",
    "- Use it to build chatbots, Generative Question-Answer (GQA), summarization and much more.\n",
    "- Core idea is to “chain” together different components to create more advanced use cases around LLMs.\n",
    "- Provides developers with a comprehensive set of tools to seamlessly combine multiple prompts working with LLMs effortlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a344c6-9289-4351-9677-8e7f778c7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain\n",
    "%pip install --quiet -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f2767-9f87-48e8-b1b7-c0b952d2cdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain        0.3.11\n",
    "# langchain-core   0.3.24\n",
    "# langchain-openai 0.2.12\n",
    "# openai           1.57.2\n",
    "# pydantic         2.10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7281bdb-a238-4cad-abac-6679aa169ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# setup the OpenAI API Key\n",
    "\n",
    "# get OpenAI API key ready and enter it when ask\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc02a79-8283-4235-b570-c813d51bff91",
   "metadata": {},
   "source": [
    "### A Simple LLM Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "892ce11d-96fc-4aad-b307-0ec50122e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb2fe6ff-5fba-46d9-a05d-ec835ce93b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why not a gpt-3.5-turbo-instruct ?\n",
    "# https://github.com/BerriAI/litellm/issues/749\n",
    "# gpt-3.5-turbo-instruct is an openai text completions model and so gets routed to completions not chat completions.\n",
    "\n",
    "# in summary you will notice the patterns:\n",
    "# model name = gpt-3.5-turbo-instruct --> text completion --> OpenAI\n",
    "# model name - gpt-3.5-turbo --> chat completion --> ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    # don't need this if the OpenAI API Key is stored in the environment variable\n",
    "    #openai_api_key=\"sk-proj-xxxxxxxxx\",\n",
    "\n",
    "    #model = 'gpt-3.5-turbo'\n",
    "    model = 'gpt-4o-mini'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f524decd-83e2-47ac-a9c0-8ce81fe1946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup message prompt\n",
    "text = \"What date is Singapore National Day?\"\n",
    "messages = [HumanMessage(content=text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2ebcf8-6c39-432a-b0a5-13eefbfd9ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singapore National Day is celebrated on August 9th each year. It commemorates Singapore's independence from Malaysia in 1965.\n"
     ]
    }
   ],
   "source": [
    "# note that Chat Model takes in message objects as input and generate message object as output\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324918b-ae6e-49ae-8eb3-edbd8569817d",
   "metadata": {},
   "source": [
    "### A Simple LLM Application With Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53266ed8-b053-4872-8622-a5949dda4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ba721d4-9855-4c15-a188-ff90b396068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise ChatModel with API key\n",
    "chat_model = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini', \n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c572494-7a0a-4df2-a7e1-e7b9a55f1740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template takes in raw user input ({input_language} and {output_language}) and \n",
    "# return a prompt that is ready to pass into a language model\n",
    "\n",
    "system_template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "\n",
    "human_template = \"{text}\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", human_template),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2005be55-3b7f-45d6-bc9e-a7538c7d0b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime la programmation.\n"
     ]
    }
   ],
   "source": [
    "# trsnslate English to French\n",
    "\n",
    "messages = chat_prompt.format_messages(\n",
    "    input_language = \"English\", \n",
    "    output_language = \"French\", \n",
    "    text = \"I love programming.\"\n",
    ")\n",
    "\n",
    "response = chat_model.invoke(messages).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da80a83a-d814-4c23-9aa8-9efa33ee7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我爱编程。\n"
     ]
    }
   ],
   "source": [
    "# translate English to Chinese\n",
    "\n",
    "messages = chat_prompt.format_messages(\n",
    "    input_language = \"English\", \n",
    "    output_language = \"Chinese\", \n",
    "    text = \"I love programming.\"\n",
    ")\n",
    "\n",
    "response = chat_model.invoke(messages).content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7520039-1b31-46ca-96da-f5af86facff2",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "- From the two sample codes you just run, you have learned how to create your first simple LLM application. \n",
    "- You learned how to work with language model(s) and how to create a prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed6301b-ede6-4653-879e-8c47cc0b0d2b",
   "metadata": {},
   "source": [
    "## PromptTemplate And LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e694c058-eb8e-4cfa-bf5c-28ebc0e75280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load langchain libraries\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f05f62e3-cd4c-44a5-8f90-313755229e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4 does not have 'instruct' model\n",
    "# The chat models like gpt-4 and gpt-4-turbo are already instruction-following models.\n",
    "# They are designed to interpret and respond effectively to instructions provided in the conversatio\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.7,\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d692ef24-ee42-4564-87ba-0002b5b191c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup template \n",
    "\n",
    "human_template = \"Write {lines} sentences about {topic}.\"\n",
    "prompt = ChatPromptTemplate.from_template(human_template)\n",
    "\n",
    "lines_topic_dict = {\n",
    "    \"lines\" : \"3\", \n",
    "    \"topic\": \"Sir Stamford Raffles\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efc8889c-e7f3-4096-a4df-14880c1a0bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sir Stamford Raffles was a British statesman and the founder of modern Singapore, establishing the strategic trading post in 1819. He played a crucial role in the expansion of British influence in Southeast Asia and was a proponent of various social and educational reforms. Raffles is also known for his interest in natural history, contributing to the study and preservation of the region's biodiversity.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 17, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_6fc10e10eb', 'finish_reason': 'stop', 'logprobs': None}, id='run-5702950a-c7c7-4143-b215-2ab0ee15e8c4-0', usage_metadata={'input_tokens': 17, 'output_tokens': 76, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without piping to StrOutputParser\n",
    "# StrOutputParser = OutputParser that parses LLMResult into the top likely string\n",
    "\n",
    "lcel_chain_01 = prompt | llm\n",
    "\n",
    "lcel_chain_01.invoke(lines_topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d3fca9-f12f-4eb8-9e29-1f4fd13f91e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sir Stamford Raffles was a British statesman and the founder of modern Singapore, establishing it as a strategic trading post for the British East India Company in 1819. He was also a prominent naturalist and played a significant role in the promotion of scientific exploration and conservation in the region. Raffles' legacy is celebrated for his contributions to Singapore's development and his efforts in fostering cultural and economic exchanges in Southeast Asia.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipe to StrOutputParesr\n",
    "\n",
    "lcel_chain_02 = prompt | llm | output_parser\n",
    "\n",
    "lcel_chain_02.invoke(lines_topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32fd62-6e98-4294-8b46-42d8d452eee6",
   "metadata": {},
   "source": [
    "# Template\n",
    "\n",
    "## Prompt Template\n",
    "Prompt templates take as input a dictionary where each key represents a variable for the prompt template to fill in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3dbbd33f-8d1e-40c9-8ecf-e39c8c36a40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='I love playing table-tennis')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    input_variables = [\"sport\"],\n",
    "    template = \"I love playing {sport}\"\n",
    ")\n",
    "\n",
    "prompt.invoke({\"sport\":\"table-tennis\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3ecd88-7b23-460d-b31c-02f3bb64e4a4",
   "metadata": {},
   "source": [
    "Usually you will initialise Prompts using `from_template`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33087d42-d382-46ef-8092-7b97e2883de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='I love visitng Japan because of its scenery')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\n",
    "    \"I love visitng {country} because of its {adjective}\"\n",
    ")\n",
    "\n",
    "prompt.invoke({\n",
    "    \"country\" : \"Japan\",\n",
    "    \"adjective\" : \"scenery\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad56989-900d-4625-bda6-17483a6c3804",
   "metadata": {},
   "source": [
    "## MessagesPlaceholder\n",
    "Prompt template is responsible for adding a list of messages in a particular place. If the user want to pass in a list of messages that could be slotted into a particular spot, we can use `MessagePlaceholder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e7c6452-6998-4cf6-824d-c6adac2a7bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi! My name is John', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "\n",
    "    # place Human Message after System Message\n",
    "    MessagesPlaceholder(\"messages\")\n",
    "])\n",
    "\n",
    "prompt.invoke({\"messages\" : [HumanMessage(content=\"Hi! My name is John\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907223f-3e31-4f2e-8037-cd4a54330226",
   "metadata": {},
   "source": [
    "# Structured Output\n",
    "## CommaSeparatedListOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "498b3688-c28e-4cb1-a8ef-e7fd8896c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.list import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a36ef677-c3fc-433e-a364-787880fd2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CommaSeparatedListOutputParser()\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4696597f-5a79-489b-b266-8f2bfebc9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    template=\"List down 5 countries that start with letter'{alphabet}'\\n{format_instructions}\",\n",
    "    input_variables=[\"alphabet\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cdebfe0-b34b-46f4-9845-2c3061042161",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ff65cf3-3ba9-4290-a67a-e53b1def1327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spain, Sweden, Switzerland, Singapore, South Africa\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(alphabet=\"S\")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1dd6119-31a5-4367-8367-1677d4734595",
   "metadata": {},
   "outputs": [],
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3255e8-ff14-44c2-a05a-f88baa86b9ce",
   "metadata": {},
   "source": [
    "## JSON\n",
    "\n",
    "### Method 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b46537b-8eb5-4ce1-b01d-dbfcb5e00eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    model_kwargs = {\"response_format\" : { \"type\": \"json_object\" } }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d68df024-89cd-4f0c-aa9c-e397993bade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"\"\"\n",
    "   Return a JSON object with two variables. The \"name\" is \"Joe Doe\" and his \"age\" is \"30\".\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fe80fae-5e92-469c-bf60-40c4eea15ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Joe Doe\",\n",
      "  \"age\": \"30\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f72af-4510-48fc-b99d-f6279ba73a1e",
   "metadata": {},
   "source": [
    "### Method 2:\n",
    "\n",
    "- JSON requires `{` and `}` for its syntax. To escape these in Python strings, use double braces {{ and }}.\n",
    "- `{name}` and `{age}` placeholders for variables remain single braces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b03db585-97a6-4207-9b30-58734a7ea5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"name\", \"age\"],\n",
    "    template=\"\"\"You are an assistant that formats responses in JSON.\n",
    "Given the following inputs:\n",
    "- Name: {name}\n",
    "- Age: {age}\n",
    "\n",
    "Respond with a JSON object in the following format:\n",
    "{{\n",
    "    \"name\": \"value\",\n",
    "    \"age\": value\n",
    "}}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7db1c103-6b0f-48cb-808b-c3e58a830328",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(name=\"John Doe\", age=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c59e6a6-fcc0-4dd9-bc26-714dbf7c8741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"John Doe\",\n",
      "    \"age\": 30\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcc8aea-ad2f-4202-a6bd-a59d01421eac",
   "metadata": {},
   "source": [
    "## Schema Definition\n",
    "\n",
    "- The output structure of a model response needs to be represented in some way.\n",
    "- The simplest and most common format is a JSON-like structure which you just seen.\n",
    "- The other method is use `Pydantic` as it allow you mto define schemas using Python's type annotations\n",
    "- It validates that the LLM's output conforms to the expected structure, catching errors early.\n",
    "\n",
    "### Method 1: (Tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02a37201-8c76-474d-8b0a-ced95c50745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ResponseFormatter(BaseModel):\n",
    "    answer: str = Field(description = \"The answer to the user's question\")\n",
    "    followup_question: str = Field(description = \"A followup question the user could ask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb769fe7-a00d-477f-8bec-9f425940d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4o\", \n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "# Bind responseformatter schema as a tool to the model\n",
    "model_with_tools = model.bind_tools([ResponseFormatter])\n",
    "\n",
    "# Invoke the model\n",
    "response = model_with_tools.invoke(\"What was the original colour of the Hulk in his first comic appearance?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "154f3594-0d48-42b3-92f0-8ac38a5bdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 'eval' method to extract followup_question\n",
    "\n",
    "followup = eval(response.additional_kwargs['tool_calls'][0]['function']['arguments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0a4cdf8-5e09-4012-b56e-9020bc81a115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why was the Hulk's color changed from gray to green?\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "followup['followup_question']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05cd37-4584-44e9-a75a-f991e92bb337",
   "metadata": {},
   "source": [
    "### Method 2: (Pydantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d20d0d72-58f5-4144-90fe-2f213b18fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.pydantic import PydanticOutputParser\n",
    "from langchain_core.prompts.chat import SystemMessagePromptTemplate\n",
    "\n",
    "class ResponseFormatter(BaseModel):\n",
    "    answer: str = Field(description = \"The answer to the user's question\")\n",
    "    followup_question: str = Field(description = \"A followup question the user could ask\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ResponseFormatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "777365ce-95c3-43be-aba2-81b1d4128eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4o\", \n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49fef609-ec20-45ec-9a18-6d4779ec28f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "prompt = ChatPromptTemplate.from_messages([system_message_prompt])\n",
    "\n",
    "messages = prompt.format_prompt(\n",
    "    format_instructions=parser.get_format_instructions(),\n",
    "    query = \"What was the original colour of the Hulk in his first comic appearance?\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "933a5d02-0bc3-4178-9ade-02db9a6079e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the model\n",
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7386a0c7-6240-4538-b3a9-29658ed944c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the parse to extract the answer and followup question\n",
    "\n",
    "output = parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "208eceb8-4ad5-46ed-abb0-5aea44ead1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The original color of the Hulk in his first comic appearance was gray.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38d3ff8c-44c1-4391-9aa5-baa6e1275f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the Hulk's color change from gray to green?\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.followup_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd447061-dc06-41c0-879c-667a6545e9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
